{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_learning + Clipping operation = DL-Clip\n",
    "# 对象为 Inverted Pendulum\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cp\n",
    "import pickle\n",
    "\n",
    "from systems_and_functions.control_affine_system import ControlAffineSystem\n",
    "from systems_and_functions.cart_pole_system import CartPole\n",
    "from systems_and_functions.inverted_pendulum_system import InvertedPendulum\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_states: int = 4, \n",
    "        n_hiddens: int = 16,\n",
    "        n_actions: int = 2, \n",
    "        action_bound: float = 5.0\n",
    "    ):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        # 环境可以接受的动作最大值\n",
    "        self.action_bound = action_bound\n",
    "        self.n_actions = n_actions\n",
    "        # 只包含一个隐含层\n",
    "        self.fc1 = nn.Linear(n_states, n_hiddens)\n",
    "        self.fc2 = nn.Linear(n_hiddens, n_actions + 1)\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        x = x.t()\n",
    "        x = self.fc1(x)  # [b,n_states]-->[b,n_hiddens]\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)  # [b,n_hiddens]-->[b,n_actions]\n",
    "        return x\n",
    "    \n",
    "    def Controller(self, x):\n",
    "        u = self.forward(x).t()[:self.n_actions]\n",
    "        return u\n",
    "\n",
    "class LyapunovNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_states:int = 4, \n",
    "        n_hiddens:int = 128\n",
    "    ):\n",
    "        super(LyapunovNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_states, n_hiddens)\n",
    "        self.fc2 = nn.Linear(n_hiddens, n_hiddens)\n",
    "        self.fc3 = nn.Linear(n_hiddens, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = self.fc1(x)  # -->[b, n_hiddens]\n",
    "        s = torch.tanh(s)\n",
    "        s = self.fc2(s)  # -->[b, n_hiddens]\n",
    "        s = torch.tanh(s)\n",
    "        V = self.fc3(s)  # -->[b, 1]\n",
    "        return V\n",
    "    \n",
    "    def V(self, x):\n",
    "        return self.forward(x)[:,0]\n",
    "\n",
    "    def V_with_JV(self, x):\n",
    "        x = x.clone().detach().requires_grad_(True)\n",
    "        V = self.forward(x)\n",
    "        JV = torch.autograd.grad(V, x)\n",
    "        return V, JV\n",
    "\n",
    "class DFunctionNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_states:int = 4, \n",
    "        n_hiddens: int = 128, \n",
    "        n_actions: int = 2\n",
    "    ):\n",
    "        super(DFunctionNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_states + n_actions, n_hiddens)\n",
    "        self.fc2 = nn.Linear(n_hiddens, n_hiddens)\n",
    "        self.fc3 = nn.Linear(n_hiddens, 2)\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x, a):\n",
    "        # 拼接状态和动作\n",
    "        cat = torch.cat([x, a], dim=1)  # [b, n_states + n_actions]\n",
    "        x = self.fc1(cat)  # -->[b, n_hiddens]\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)  # -->[b, n_hiddens]\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)  # -->[b, 1]\n",
    "        return x\n",
    "\n",
    "    def D(self, x, a):\n",
    "        return self.forward(x, a)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D-learning 类，包含了D-learning中的各种功能函数 \n",
    "class DlearningProcess:\n",
    "    def __init__(\n",
    "        self, \n",
    "        system: ControlAffineSystem,\n",
    "        actor_bound: float = 5.0,\n",
    "        n_hiddens_policy: int = 16,\n",
    "        n_hiddens_lyapunov: int = 128,\n",
    "        n_hiddens_dfunction: int = 128,\n",
    "        actor_lr: float = 0.01,\n",
    "        lyapunov_lr: float = 0.01,\n",
    "        dfunction_lr: float = 0.01,\n",
    "        sigma: float = 0.5,\n",
    "        tau: float = 0.1,\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'),\n",
    "    ):\n",
    "        # 属性分配\n",
    "        self.system = system # 动力学系统\n",
    "        self.sigma = sigma # 噪声标准差\n",
    "        self.tau = tau # 软更新权重\n",
    "        self.n_states = self.system.state_dims()\n",
    "        self.n_actions = self.system.control_dims()\n",
    "        self.device = device\n",
    "        # 训练网络\n",
    "        self.actor = PolicyNet(self.system.state_dims(), n_hiddens_policy, self.system.control_dims(), actor_bound).to(device)\n",
    "        self.lyapunov = LyapunovNet(self.system.state_dims(), n_hiddens_lyapunov).to(device)\n",
    "        self.dfunction = DFunctionNet(self.system.state_dims(), n_hiddens_dfunction, self.system.control_dims()).to(device)\n",
    "        # 训练网络的优化器\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.lyapunov_optimizer = torch.optim.Adam(self.lyapunov.parameters(), lr=lyapunov_lr)\n",
    "        self.dfunction_optimizer = torch.optim.Adam(self.dfunction.parameters(), lr=dfunction_lr)\n",
    "\n",
    "        # DONE: 初始化神经网络控制器\n",
    "\n",
    "\n",
    "    def initialize_policy_net(\n",
    "            self,\n",
    "            x_train_lim: int = 10,\n",
    "            x_test_lim: int = 13,\n",
    "            sample_num: int = 1000,\n",
    "            iteration: int = 2*10**4,\n",
    "            lr: float = 1e-4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        用神经网络拟合专家策略（线性静态状态反馈）\n",
    "        \"\"\"\n",
    "        print('---------------------Initializing Policy------------------------')\n",
    "\n",
    "        random_data = [-x_train_lim + torch.rand(400)*2*x_train_lim for _ in range(self.system.state_dims())]\n",
    "        train_data = torch.stack(random_data, dim=1).to(self.device)\n",
    "        K = torch.tensor(-self.system.K)\n",
    "        zero_column = torch.zeros(1, 2)\n",
    "        K_extended = torch.vstack((K, zero_column)).t().to(self.device)\n",
    "        labels = train_data @ K_extended\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD(self.actor.parameters(), lr = lr)\n",
    "        for i in range(iteration):\n",
    "            optimizer.zero_grad()\n",
    "            y_train = self.actor(train_data.t())\n",
    "            loss = loss_fn(y_train, labels)\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print ('Epoch [{}/{}], Loss: {:.10f}'.format(i + 1, iteration, loss.item()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    \n",
    "    \n",
    "    def sample_training_data(\n",
    "        #  沿轨迹采样 + 均匀分布采样 \n",
    "        self,\n",
    "        sample_trajectory_number: int = 10,\n",
    "        sample_number_per_trajectory: int = 500,\n",
    "        sample_radius: int = 15,\n",
    "        sample_number_in_radius: int = 0,\n",
    "        invariant_sample: bool = True,\n",
    "        sample_plot: bool = True,\n",
    "        the_controller = None, # 指定控制器，默认为ControlAffineSystem自带控制器\n",
    "        title = \"Samples\"\n",
    "    )->torch.Tensor:\n",
    "        \"\"\"\n",
    "        sample training data\n",
    "        data content: x_i, x_dot_i, controller_params\n",
    "        sample strategy: data set is composed of two parts: \n",
    "            samples from trajectory;\n",
    "            samples from random state;\n",
    "            (if invariant_sample is true, random state point is Unchanging) \n",
    "        \"\"\"\n",
    "        # initialize system: use LQR as \n",
    "        # self.system.compute_LQR_controller()\n",
    "        # self.system.use_LQR_controller()\n",
    "\n",
    "        # sample initialize ([100, 2, 2, 1])\n",
    "        # dim1: sample index\n",
    "        # dim2: x  x'\n",
    "        # dim3: x1 x2\n",
    "        # dim4: value\n",
    "        sample_from_trajectory = torch.zeros(sample_trajectory_number*sample_number_per_trajectory,2,2,1).to(self.device)\n",
    "        sample_from_radius = torch.zeros(sample_number_in_radius,2,2,1).to(self.device)\n",
    "        theta = np.linspace(0, 2*np.pi, sample_trajectory_number + 1)\n",
    "        \n",
    "        # sample in trajectory\n",
    "        for i in range(0, sample_trajectory_number):\n",
    "            x_0_traj = torch.tensor([[sample_radius*np.cos(theta[i])],[sample_radius*np.sin(theta[i])]],dtype=torch.float).to(self.device)\n",
    "            simulate_rk4 = self.system.simulate_rk4(x_0_traj,sample_number_per_trajectory,1,the_controller)\n",
    "            x = simulate_rk4[:,0].unsqueeze(1)\n",
    "            x_dot = simulate_rk4[:,1].unsqueeze(1)\n",
    "            sample_from_trajectory[i*sample_number_per_trajectory:(i+1)*sample_number_per_trajectory] = torch.cat((x, x + x_dot*self.system.dt), dim=1)\n",
    "\n",
    "        # sample randomly in radius\n",
    "        if invariant_sample == True:\n",
    "            np.random.seed(42)\n",
    "        theta_ = np.random.uniform(0, 2*np.pi, sample_number_in_radius)\n",
    "        r_ = np.sqrt(np.random.uniform(0, sample_radius**2, sample_number_in_radius))\n",
    "        combined_data = zip(theta_, r_)\n",
    "        i = 0\n",
    "        for data in combined_data:\n",
    "            theta__, r__ = data\n",
    "            x_0_radius = torch.tensor([[r__ * np.cos(theta__)],[r__ * np.sin(theta__)]]).to(self.device)\n",
    "            one_step_euler = self.system.one_step_euler(x_0_radius,1,the_controller)[1].to(self.device)\n",
    "            x = one_step_euler[0].unsqueeze(0)\n",
    "            x_dot = one_step_euler[1].unsqueeze(0)\n",
    "            sample_from_radius[i] = torch.cat((x, x + x_dot*self.system.dt), dim=0)\n",
    "            i = i + 1\n",
    "\n",
    "        sample_data = torch.cat((sample_from_trajectory,sample_from_radius),dim=0).to(self.device)\n",
    "        return sample_data\n",
    "\n",
    "\n",
    "    def learn_V_LQF(\n",
    "        self,\n",
    "        sample_data,\n",
    "        plot_lyapuonv: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        learn a V(x) by constraining V(x)>=0, V_dot(x)<=0\n",
    "        \"\"\"\n",
    "        print('--------------------------Learning V--------------------------')\n",
    "        sample_data = sample_data.cpu().detach().numpy()\n",
    "        P = cp.Variable((self.system.state_dims(), self.system.state_dims()))\n",
    "        eta = cp.Variable()\n",
    "        \n",
    "        constraints = [P.T == P, eta>=0]\n",
    "        for i in range(sample_data.shape[0]):\n",
    "            xi = sample_data[i][0]\n",
    "            xi_dot = (sample_data[i][1]-sample_data[i][0])/self.system.dt\n",
    "            constraints.append(xi.T@P@xi>=0)\n",
    "            constraints.append(xi.T@P@xi_dot+xi_dot.T@P@xi<=-eta*np.linalg.norm(xi)**2)\n",
    "\n",
    "        objective = cp.Minimize(-eta)\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        # 求解优化问题\n",
    "        problem.solve()\n",
    "        print(\"status:\",problem.status)\n",
    "        # print(\"optimal value\",problem.value)\n",
    "        # 输出解\n",
    "        print(\"var eta (eta should be positive):\", eta.value)\n",
    "        print(\"var P:\", P.value)\n",
    "        self.P = torch.tensor(P.value).float()\n",
    "\n",
    "        if plot_lyapuonv:\n",
    "            xlim = 5\n",
    "            ylim = 5\n",
    "            print('-----------------------Plotting Lyapunov-----------------------')\n",
    "            x = np.linspace(-xlim, xlim, 100)\n",
    "            y = np.linspace(-ylim, ylim, 100)\n",
    "            X, Y = np.meshgrid(x, y)\n",
    "            grid = np.stack([X, Y], axis=-1)\n",
    "            P = self.P\n",
    "            Z = np.einsum('...i,ij,...j->...', grid, P, grid)\n",
    "\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.contourf(X, Y, Z, levels=25, cmap='RdBu')\n",
    "            plt.colorbar(label='Lyapunov Function')\n",
    "            plt.xlabel(r\"$\\mathregular{x_{1}}$\")\n",
    "            plt.ylabel(r\"$\\mathregular{x_{2}}$\")\n",
    "            plt.title('Lyapunov Function Contour Plot')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def V_value_P_batch(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x:[sample num, value, n dims]\n",
    "        return [sumple num, value]\n",
    "        \"\"\"\n",
    "        P = self.P.to(self.device)\n",
    "        xP = torch.matmul(x, P)\n",
    "        quadratic_form = torch.sum(xP * x, dim=1, keepdim=True)\n",
    "\n",
    "        return quadratic_form\n",
    "\n",
    "\n",
    "    def initialize_lyapunov_net(\n",
    "            self,\n",
    "            x_train_lim: int = 15,\n",
    "            x_test_lim: int = 13,\n",
    "            sample_num: int = 1000,\n",
    "            iteration: int = 5*10**4,\n",
    "            lr: float = 1e-3,\n",
    "            plot_loss: bool = True,\n",
    "            plot_lyapuonv: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        用lyapunov_net拟合二次型lyapunov作为初始化\n",
    "        \"\"\"\n",
    "        print('--------------------Initializing Lyapunov---------------------')\n",
    "        random_data = [-x_train_lim + torch.rand(sample_num) * 2 * x_train_lim for _ in range(self.system.state_dims())]\n",
    "        training_data = torch.stack(random_data, dim=1).to(self.device)\n",
    "        V_value_P = self.V_value_P_batch(training_data)\n",
    "        labels = torch.cat([V_value_P, torch.zeros(V_value_P.shape[0],1).to(self.device)], dim=1).to(self.device)\n",
    "\n",
    "        loss_fn = nn.MSELoss()\n",
    "        # optimizer = self.lyapunov_optimizer\n",
    "        # optimizer = torch.optim.Adam(self.lyapunov.parameters(), lr = lr)\n",
    "        optimizer = torch.optim.Adam(self.lyapunov.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=0)\n",
    "        loss_values = []\n",
    "        for i in range(iteration):\n",
    "            optimizer.zero_grad()\n",
    "            y_train = self.lyapunov(training_data)\n",
    "            param_squares = [p ** 2 for p in self.lyapunov.parameters()]\n",
    "            param_sum_square = sum(torch.sum(p) for p in param_squares)\n",
    "            loss = loss_fn(y_train, labels) + 0.01 * param_sum_square\n",
    "            # if i % 10000 == 0:\n",
    "            #     print(f'times {i} - lr {lr} -  loss: {loss.item()}')\n",
    "            loss_values.append(loss.item())\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print ('Epoch [{}/{}], Loss: {:.10f}'.format(i + 1, iteration, loss.item()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if plot_loss:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(loss_values, label='Loss')\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Initializing V Loss')\n",
    "            plt.legend() \n",
    "            plt.show()\n",
    "\n",
    "        if plot_lyapuonv:\n",
    "            self.plot_contour(5, 5, 'Lyapunov')\n",
    "\n",
    "    # DONE: learn a NN Lyapunov Candidate using discrete data\n",
    "    def learn_V_LNN(\n",
    "        self,\n",
    "        sample_data,\n",
    "        iteration: int = 3*10**3,\n",
    "        plot_loss: bool = True,\n",
    "        plot_lyapuonv: bool = True,\n",
    "        lr: float = 1e-4\n",
    "    ):\n",
    "        dt = self.system.dt\n",
    "        print('--------------------------Learning V--------------------------')\n",
    "        sample_data = sample_data.detach().clone()\n",
    "        # DONE: using sample_data = sample_data.detach().clone()\n",
    "        # RuntimeError: Trying to backward through the graph a second time \n",
    "        # (or directly access saved tensors after they have already been freed).\n",
    "        # Saved intermediate values of the graph are freed when you call \n",
    "        # .backward() or autograd.grad(). Specify retain_graph=True \n",
    "        # if you need to backward through the graph a second time or \n",
    "        # if you need to access saved tensors after calling backward\n",
    "\n",
    "        # sample initialize ([100, 2, 2, 1])\n",
    "        # dim1: sample index\n",
    "        # dim2: x  x'\n",
    "        # dim3: x1 x2\n",
    "        # dim4: value\n",
    "\n",
    "        N = sample_data.shape[0]\n",
    "        s = sample_data[:,0].permute(0, 2, 1).squeeze(1)\n",
    "        s_ = sample_data[:,1].permute(0, 2, 1).squeeze(1)\n",
    "        s0 = torch.tensor([[0.],[0.]]).t().to(device)\n",
    "        optimizer = torch.optim.Adam(self.lyapunov.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=0)\n",
    "        # self.lyapunov_optimizer\n",
    "        loss_values = []\n",
    "\n",
    "        \n",
    "        # 扩大 ROA\n",
    "        # Circle_Tuning = Tune(sample_data[:,0]).squeeze()\n",
    "        data = sample_data[:,0]\n",
    "        result = []\n",
    "        for j in range(data.shape[0]):\n",
    "            row_sum = torch.sum(data[j,:]**2)\n",
    "            f = torch.sqrt(row_sum)\n",
    "            result.append(f.item())  # 将结果添加到列表中，使用item()获取标量值\n",
    "        Circle_Tuning = torch.tensor(result)  # 扩大 ROA  经验李雅普诺夫损失中的 tunning term\n",
    "        Circle_Tuning = Circle_Tuning.squeeze()\n",
    "        for i in range(iteration):\n",
    "            L0 = self.lyapunov.V(s0)\n",
    "            Ls = self.lyapunov.V(s)\n",
    "            Ls_ = self.lyapunov.V(s_)\n",
    "            dL = (Ls_ - Ls) / dt\n",
    "            SND = torch.sum(F.relu(dL+0.5))\n",
    "            PD = torch.sum(F.relu(-Ls))\n",
    "\n",
    "            Ls_squeezed = Ls.squeeze()\n",
    "            loss = (PD + 1.5 * SND)/N + 1.2* L0.pow(2) + 2.2*((Circle_Tuning-6*Ls_squeezed).pow(2)).mean()\n",
    "            # 经验李雅普诺夫损失\n",
    "            loss.backward(retain_graph = True)  # 计算损失的梯度\n",
    "            with torch.no_grad(): \n",
    "                optimizer.step()                # 根据梯度更新模型参数\n",
    "                optimizer.zero_grad()           # 清除梯度，准备下一个迭代\n",
    "            loss_values.append(loss.item())\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print ('Epoch [{}/{}], Loss: {:.10f}'.format(i + 1, iteration, loss.item()))\n",
    "\n",
    "        if plot_loss:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(loss_values, label='Loss')\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training V Loss')\n",
    "            plt.legend() \n",
    "            plt.show()\n",
    "        \n",
    "        if plot_lyapuonv:\n",
    "            self.plot_contour()\n",
    "        print('L0:{}, PD:{}, torch.sum(F.relu(dL)):{}'.format(L0, PD, torch.sum(F.relu(dL)) ))\n",
    "    \n",
    "    def plot_contour(\n",
    "        self,\n",
    "        xlim = 5,\n",
    "        ylim = 5,\n",
    "        select = 'Lyapunov'\n",
    "    ):\n",
    "        print('-----------------------Plotting Lyapunov-----------------------')\n",
    "        x = np.linspace(-xlim, xlim, 100)\n",
    "        y = np.linspace(-ylim, ylim, 100)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = np.zeros(X.shape)\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "            # 将X和Y的值合并成一个二维列向量\n",
    "                xy = torch.tensor([[X[i, j], Y[i, j]]], dtype=torch.float32).to(device)\n",
    "                # 计算Lyapunov函数值\n",
    "                if select == 'Dfunction':\n",
    "                    Z[i, j] = self.dfunction.D(xy,self.actor.Controller(xy.t())).item()\n",
    "                else:\n",
    "                    Z[i, j] = self.lyapunov.V(xy).item()\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.contourf(X, Y, Z, levels=25, cmap='RdBu')\n",
    "        plt.colorbar(label='Function Value')\n",
    "        plt.xlabel(r\"$\\mathregular{x_{1}}$\")\n",
    "        plt.ylabel(r\"$\\mathregular{x_{2}}$\")\n",
    "        plt.title(select +' Function Contour Plot')\n",
    "        plt.show()\n",
    "\n",
    "    def learn_D_DNN(\n",
    "        self,\n",
    "        sample_data,\n",
    "        iteration: int = 10**4,\n",
    "        plot_loss: bool = True,\n",
    "        plot_dfunction: bool = True,\n",
    "        lr: float = 1e-4\n",
    "    ):\n",
    "        dt = self.system.dt\n",
    "        print('--------------------------Learning D--------------------------')\n",
    "        sample_data = sample_data.detach().clone()\n",
    "        N = sample_data.shape[0]\n",
    "        s = sample_data[:,0].permute(0, 2, 1)\n",
    "        s_ = sample_data[:,1].permute(0, 2, 1)\n",
    "        s0 = torch.tensor([[0.],[0.]]).to(device)\n",
    "        a0 = self.actor.Controller(s0)\n",
    "        Ls = self.lyapunov(s)\n",
    "        Ls_ = self.lyapunov(s_)\n",
    "        dL = ((Ls_ - Ls) / dt).squeeze(1)\n",
    "        print(dL.shape)\n",
    "        a = self.actor.Controller(s.squeeze(1).t()).t()\n",
    "        # optimizer = self.dfunction_optimizer\n",
    "        optimizer = torch.optim.Adam(self.dfunction.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=0)\n",
    "        loss_values = []\n",
    "        loss_fn = nn.MSELoss()\n",
    "        for i in range(iteration):\n",
    "            D0 = self.dfunction.D(s0.t(), a0)\n",
    "            DV_ext = self.dfunction(s.squeeze(1),a)\n",
    "            DV = self.dfunction.D(s.squeeze(1),a)\n",
    "            param_squares = [p ** 2 for p in self.dfunction.parameters()]\n",
    "            param_sum_square = sum(torch.sum(p) for p in param_squares)\n",
    "            loss = torch.sum(loss_fn(dL, DV_ext)) + torch.sum(F.relu(DV)) + D0**2 + 0.01 * param_sum_square\n",
    "            loss.backward(retain_graph = True)\n",
    "            with torch.no_grad(): \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            loss_values.append(loss.item())\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print ('Epoch [{}/{}], Loss: {:.10f}'.format(i + 1, iteration, loss.item()))\n",
    "        \n",
    "        if plot_loss:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(loss_values, label='Loss')\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training D Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        if plot_dfunction:\n",
    "            self.plot_contour(select='Dfunction')\n",
    "        print('torch.sum(loss_fn(dL, DV_ext)):{}, torch.sum(F.relu(DV)):{}, D0:{}'.format(torch.sum(loss_fn(dL, DV_ext)),torch.sum(F.relu(DV)),D0))\n",
    "\n",
    "    def upper_bound_loss(self, output, K0):\n",
    "        positive_penalty = torch.sum(torch.relu(output + 0.1))\n",
    "        control_effort_penalty = torch.sum(self.Learned_K**2)\n",
    "        upper_bound = torch.max(output, dim=0).values\n",
    "        control_deviation_penalty = torch.sum((self.Learned_K - K0)**2)\n",
    "        # print('upper_bound:{}, positive_penalty:{}'.format(upper_bound, positive_penalty))\n",
    "        return upper_bound + positive_penalty + control_effort_penalty*0 + control_deviation_penalty*0\n",
    "    \n",
    "    def mean_variance_loss(self,output):\n",
    "        positive_penalty = torch.sum(torch.relu(output))\n",
    "        mean = torch.mean(output)\n",
    "        variance = torch.var(output)\n",
    "        # print('Mean:{}, Variance:{}'.format(mean, variance))\n",
    "        return mean + variance*0.1 + positive_penalty*1000\n",
    "\n",
    "    def upper_bound_mean_variance_loss(self,output,temp):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        positive_penalty = torch.sum(torch.relu(output))\n",
    "        # upper_bound = torch.max(output, dim=0).values\n",
    "        upper_bound = torch.max(temp, dim=0).values\n",
    "        mean = torch.mean(output)\n",
    "        variance = torch.var(output)\n",
    "        return upper_bound*100 + mean*10 + variance*0 + positive_penalty*1000\n",
    "\n",
    "\n",
    "    def imitate_PPO(\n",
    "        self,\n",
    "        N,\n",
    "        a_new,\n",
    "        a_old    \n",
    "    ):\n",
    "        Epsilon = 0.2\n",
    "        # Calculate ratio for each element\n",
    "        ratio = torch.norm(a_new / a_old, dim=1)  # dim=1 to calculate norm along rows\n",
    "        # Update a_new if ratio condition is met\n",
    "        for i in range(N):\n",
    "            if ratio[i] <= 1 - Epsilon:\n",
    "                a_new[i] = a_old[i] * (1 - Epsilon)\n",
    "            elif ratio[i] >= 1 + Epsilon:\n",
    "                a_new[i] = a_old[i] * (1 + Epsilon)\n",
    "            else:\n",
    "                # No action needed if ratio is within bounds\n",
    "                pass\n",
    "        return a_new\n",
    "\n",
    "            \n",
    "    # TODO: 策略改进\n",
    "    def policy_improvement_PPO(\n",
    "        self,\n",
    "        sample_data,\n",
    "        iteration: int = 10**4,\n",
    "        plot_loss: bool = True,\n",
    "        lr: float = 1e-4,\n",
    "    ):\n",
    "        print('----------------------Improveing Policy-----------------------')\n",
    "        sample_data = sample_data.detach().clone()\n",
    "        N = sample_data.shape[0]\n",
    "        s = sample_data[:,0].permute(0, 2, 1).squeeze(1)\n",
    "        # a = self.actor.Controller(s.t()).t()\n",
    "        # DV = self.dfunction.D(s,a)\n",
    "        # optimizer = self.actor_optimizer\n",
    "        optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=0)\n",
    "        loss_values = []\n",
    "        \n",
    "        a_old = self.actor.Controller(s.t()).t()\n",
    "\n",
    "        for i in range(iteration):\n",
    "  \n",
    "            a_new = self.actor.Controller(s.t()).t()\n",
    "            a = self.imitate_PPO(N,a_new,a_old)\n",
    "     \n",
    "            DV = self.dfunction.D(s.squeeze(1),a)\n",
    "      \n",
    "            s_squared = torch.pow(s, 2)\n",
    "            s_sum = torch.sum(s_squared, dim=1, keepdim=True) \n",
    "            s_sum = s_sum.squeeze()\n",
    "\n",
    "            # 检查s_sum中是否有元素为0\n",
    "            zero_indices = (s_sum == 0)\n",
    "            # 将为0的元素设置为10^-6\n",
    "            s_sum[zero_indices] = 1e-6\n",
    "            \n",
    "            DV_s = DV / s_sum\n",
    "        \n",
    "            param_squares = [p ** 2 for p in self.actor.parameters()]\n",
    "            param_sum_square = sum(torch.sum(p) for p in param_squares)\n",
    "            loss = self.upper_bound_mean_variance_loss(DV,DV_s) + 0.01 * param_sum_square\n",
    "            loss.backward(retain_graph = True)\n",
    "\n",
    "            with torch.no_grad(): \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            loss_values.append(loss.item())\n",
    "\n",
    "            # Update a_old for the next iteration\n",
    "            a_old = a.detach().clone()\n",
    "\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print ('Epoch [{}/{}], Loss: {:.10f}'.format(i + 1, iteration, loss.item()))\n",
    "\n",
    "        if plot_loss:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(loss_values, label='Loss')\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Policy Improvement Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controller is involved.\n",
      "-----------------It takes 226 steps to converge.------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAHICAYAAACvYErQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABKBElEQVR4nO3dd3xT9d4H8E+SNulO96KDtkDZBQpU9qZyFUEUr+MiS8SBC8eFxwHqVVxcuXJ9AL0+oLhBHFdFhiCzIkM2BUopo4XuNp1pm5znjzSB0lKatunJOefzfr3y0qZJzrfJ6Ydfv+d3fkclCIIAIiKSNbXYBRARkeMx7ImIFIBhT0SkAAx7IiIFYNgTESkAw56ISAEY9kRECsCwJyJSAIY9EZECMOzbgEqlwpw5c8QuQ5FUKhUWLlzo1Nttyf6RkZEBlUqFVatWNev59tTZvn17TJs2ze5ttLRGKVu1ahVUKhUyMjJs9w0fPhzDhw9v81rsDvsjR47gzjvvRHR0NNzc3NCuXTuMGTMGS5cudUR9OH78OBYuXFjnzXIWKpXKdlOr1QgPD8fYsWPx22+/iV1ai7XVz/bzzz+3aRjv3r0bCxcuRFFRUZttU8zt2ksqdbamtt4HRSPYYdeuXYJWqxU6dOggvPrqq8KHH34ovPTSS8LYsWOFuLg4e16qydasWSMAELZu3eqQ128JAMKYMWOE1atXC5988onw8ssvCyEhIYJKpRJ+/vnnOo979NFHRazUfk392Vrq0UcfFezcDe1SUVEhVFdX275+++23BQDC2bNnHbZNe7fbkv3DbDYLFRUVQk1NjcPrrKysFKqqquzextmzZwUAwsqVK5tVo6M5ch9cuXJlvfdz2LBhwrBhwxyyvca42PMPw2uvvQa9Xo+9e/fC19e3zvdycnJa+u+OJHXq1Al/+9vfbF/ffvvt6NmzJ5YsWYJx48aJWFnLOfJnKysrg6enp13PqampgdlshlarbfJz3Nzc7C2tVbTVdlUqVYu2Zc9zdTpds7cjF83ZBx3JbDajqqqqSZ+jXW2cM2fOoFu3bvWCHgCCg4Nt/z9s2DAkJCQ0+Brx8fFITk62ff3ll18iMTER3t7e8PHxQY8ePfCvf/0LgKXfNXnyZADAiBEjbG2Fq1sJ69evx5AhQ+Dp6Qlvb2/ccsstOHbsWJ1tTps2DV5eXjh//jxuvfVWeHl5oV27dnj//fcBWFpTI0eOhKenJ6Kjo/H555/b87bU0aNHDwQGBuLs2bP1vvfdd9+he/fu0Ol06NatG3755Zc63z937hweeeQRxMfHw93dHQEBAZg8eXK9FlZ1dTVefvlldOzYEW5ubggICMDgwYOxadOmOo9LTU3FnXfeCX9/f7i5uaFv37744YcfWvVn27Jli+399/X1xYQJE3DixIk6z1u4cCFUKhWOHz+Oe++9F35+fhg8eDCmTZtm+wyubhsBV/q877zzDpYsWYK4uDjodDocP34cVVVVeOmll5CYmAi9Xg9PT08MGTIEW7durVfz1T3phQsX4tlnnwUAxMTE2LZ3vRbhe++9B41GU6elsXjxYqhUKsydO9d2n8lkgre3N/7+97+3aLs32j8a0lA/3Lq/Z2ZmYuLEifDy8kJQUBCeeeYZmEymZr8/1/bsCwoK8Mwzz6BHjx7w8vKCj48Pxo0bh0OHDt2w7uspKirCU089hfbt20On0yEiIgL3338/8vLybI/JycnBzJkzERISAjc3NyQkJODjjz9u8H1555138MEHH9j2n379+mHv3r113qvm7INA0/b9pjIajViwYAE6dOgAnU6HyMhIPPfcczAajXUeZz2+89lnn6Fbt27Q6XRN2k8AwK6RfXR0NFJSUnD06FF07979uo+bMmUKZs2aVe9xe/fuxalTp/DCCy8AADZt2oR77rkHo0aNwptvvgkAOHHiBHbt2oUnnngCQ4cOxeOPP4733nsP//M//4MuXboAgO2/q1evxtSpU5GcnIw333wT5eXlWLZsGQYPHow///wT7du3t23bZDJh3LhxGDp0KN566y189tlnmDNnDjw9PfH888/jvvvuw6RJk7B8+XLcf//9GDBgAGJiYux5ewAAhYWFKCwsRIcOHercv3PnTqxbtw6PPPIIvL298d577+GOO+7A+fPnERAQYHt/du/ejbvvvhsRERHIyMjAsmXLMHz4cBw/fhweHh4ALL+UixYtwgMPPID+/fvDYDBg3759OHDgAMaMGQMAOHbsGAYNGoR27dph3rx58PT0xNdff42JEyfim2++we23397in23z5s0YN24cYmNjsXDhQlRUVGDp0qUYNGgQDhw4UOf9B4DJkyejY8eOeP311yEIAnr37o2srCxs2rQJq1evbnCbK1euRGVlJR588EHodDr4+/vDYDDgP//5D+655x7MmjULJSUl+Oijj5CcnIw//vgDvXr1avC1Jk2ahFOnTuGLL77Au+++i8DAQABAUFBQg48fMmQIzGYzdu7ciVtvvRUAsGPHDqjVauzYscP2uD///BOlpaUYOnRos7fblP3DHiaTCcnJyUhKSsI777yDzZs3Y/HixYiLi8PDDz/cKu9Peno6vvvuO0yePBkxMTHIzs7GihUrMGzYMBw/fhzh4eF21VxaWoohQ4bgxIkTmDFjBvr06YO8vDz88MMPuHjxIgIDA1FRUYHhw4cjLS0Nc+bMQUxMDNasWYNp06ahqKgITzzxRJ3X/Pzzz1FSUoLZs2dDpVLhrbfewqRJk5Ceng5XV1fMnj27Wfugvft+Y8xmM2677Tbs3LkTDz74ILp06YIjR47g3XffxalTp/Ddd9/VefyWLVvw9ddfY86cOQgMDGz6tuzp+WzcuFHQaDSCRqMRBgwYIDz33HPChg0b6vXxioqKBDc3N+Hvf/97nfsff/xxwdPTUygtLRUEQRCeeOIJwcfHp9F+4/V69iUlJYKvr68wa9asOvdfvnxZ0Ov1de6fOnWqAEB4/fXXbfcVFhYK7u7ugkqlEr788kvb/ampqQIAYcGCBTd8PwAIM2fOFHJzc4WcnBxhz549wqhRowQAwuLFi+s8TqvVCmlpabb7Dh06JAAQli5daruvvLy83jZSUlIEAMInn3xiuy8hIUG45ZZbGq1t1KhRQo8ePYTKykrbfWazWRg4cKDQsWPHVvnZevXqJQQHBwv5+fl1fi61Wi3cf//9tvsWLFggABDuueeeetu5Xr/U2uf18fERcnJy6nyvpqZGMBqNde4rLCwUQkJChBkzZtT7Oa7+LO3p2ZtMJsHHx0d47rnnBEGwvH8BAQHC5MmTBY1GI5SUlAiCIAj//Oc/BbVaLRQWFjZru03dPxrSUD/cur+/8sordR7bu3dvITExsd62m1pndHS0MHXqVNvXlZWVgslkqlePTqers+2m9uxfeuklAYCwbt26et8zm82CIAjCkiVLBADCp59+avteVVWVMGDAAMHLy0swGAx1thkQECAUFBTYHvv9998LAIT//ve/tvuasw82dd9vSs9+9erVglqtFnbs2FFnG8uXLxcACLt27bLdB0BQq9XCsWPH6tV7I3a1ccaMGYOUlBTcdtttOHToEN566y0kJyejXbt2ddoDer0eEyZMwBdffAGh9tooJpMJX331FSZOnGjr1fr6+qKsrKxe+6EpNm3ahKKiItxzzz3Iy8uz3TQaDZKSkhr8k/6BBx6w/b+vry/i4+Ph6emJu+66y3Z/fHw8fH19kZ6e3qQ6PvroIwQFBSE4OBhJSUnYtWsX5s6diyeffLLO40aPHo24uDjb1z179oSPj0+d7bi7u9v+v7q6Gvn5+ejQoQN8fX1x4MCBOrUfO3YMp0+fbrCmgoICbNmyBXfddRdKSkps701+fj6Sk5Nx+vRpZGZmtuhnu3TpEg4ePIhp06bB39+/zs81ZswY/Pzzz/Ve76GHHrrhNq91xx131BtZajQaW8/UbDajoKAANTU16Nu3b533qaXUajUGDhyI7du3A7D81Zmfn4958+ZBEASkpKQAsIz2u3fv3mB7s6masn/Y69r3e8iQIS16vWvpdDqo1ZYIMZlMyM/Ph5eXF+Lj45v1OXzzzTdISEho8K9Oa2vl559/RmhoKO655x7b91xdXfH444+jtLQU27Ztq/O8v/71r/Dz87N9PWTIEACw6324dh9szr7fmDVr1qBLly7o3LlznSwbOXIkANTLsmHDhqFr1652bQNoxtTLfv36Yd26dSgsLMQff/yB+fPno6SkBHfeeaetlwUA999/P86fP2/7c3fz5s3Izs7GlClTbI955JFH0KlTJ4wbNw4RERGYMWNGk/tP1qAbOXIkgoKC6tw2btxY74Cxm5tbvdDQ6/WIiIiw7UhX319YWNikOiZMmIBNmzZh8+bN2LNnD/Ly8rB48WLbL4FVVFRUvef6+fnV2U5FRQVeeuklREZGQqfTITAwEEFBQSgqKkJxcbHtca+88gqKiorQqVMn9OjRA88++ywOHz5s+35aWhoEQcCLL75Y771ZsGABgKYdUG/sZzt37hwAyz+O1+rSpQvy8vJQVlZW5/7mtMWu95yPP/4YPXv2tB2zCAoKwk8//VTnfWoNQ4YMwf79+1FRUYEdO3YgLCwMffr0QUJCgm3f3rlzpy1Emqsp+4c9GtrfW/J6DTGbzXj33XfRsWPHOvvr4cOHm/U5nDlzptH2MGA5rtWxY8d6v1/W1q51v7S69n21Br8978O1+2Bz9v3GnD59GseOHav3u9qpUycA9X9Xm/N7BNjZs7+aVqtFv3790K9fP3Tq1AnTp0/HmjVrbGGSnJyMkJAQfPrppxg6dCg+/fRThIaGYvTo0bbXCA4OxsGDB7FhwwasX78e69evx8qVK3H//ffXO+ByLbPZDMDStw8NDa3/g7nU/dE0Gk2Dr3O9+4UmXq0xIiKizs90PU3ZzmOPPYaVK1fiySefxIABA6DX66FSqXD33Xfbfl4AGDp0KM6cOYPvv/8eGzduxH/+8x+8++67WL58OR544AHbY5955pk6B8Ovdu0xhZb8bE119V8uLXnOp59+imnTpmHixIl49tlnERwcDI1Gg0WLFuHMmTOtUarN4MGDUV1djZSUFOzYscMW6kOGDMGOHTuQmpqK3NzcFod9S/fDpr5ea3r99dfx4osvYsaMGXj11Vfh7+8PtVqNJ598ss7+KqbWeF+bs9/aw2w2o0ePHvjnP//Z4PcjIyNbpZ5mh/3V+vbtC8Dy542VRqPBvffei1WrVuHNN9/Ed999h1mzZtV787VaLcaPH4/x48fDbDbjkUcewYoVK/Diiy+iQ4cO9UbdVtY/eYODg1s1kMS0du1aTJ06FYsXL7bdV1lZ2eAJLv7+/pg+fTqmT59uOzi4cOFCPPDAA4iNjQVg+fPWUe9NdHQ0AODkyZP1vpeamorAwMAmTa283ufbmLVr1yI2Nhbr1q2r83zrQKM1t9e/f39otVrs2LEDO3bssM1WGTp0KD788EP8+uuvtq9bc7tisafOtWvXYsSIEfjoo4/q3F9UVGQ7uGuPuLg4HD16tNHHREdH4/DhwzCbzXVG96mpqbbv28vez6a19n2ruLg4HDp0CKNGjXLofmJXG2fr1q0N/oto7VFd+2fNlClTUFhYiNmzZ6O0tLTOnG0AyM/Pr1uMWo2ePXsCgG3KkfVNuzbwkpOT4ePjg9dffx3V1dX1asrNzbXjJ3MOGo2m3vu7dOnSetPlrn3fvLy80KFDB9t7FhwcjOHDh2PFihV1/gG2ao33JiwsDL169cLHH39c57M5evQoNm7ciL/85S9Nep3rfb6NsQ4Yrn6v9uzZY+uht+b23Nzc0K9fP3zxxRc4f/58nZF9RUUF3nvvPcTFxSEsLKxVtysWe+psaH9ds2ZNk44HNeSOO+7AoUOH8O2339b7nnU7f/nLX3D58mV89dVXtu/V1NRg6dKl8PLywrBhw+zerr2fTWvt+1Z33XUXMjMz8eGHH9b7XkVFhV0tocbYNbJ/7LHHUF5ejttvvx2dO3dGVVUVdu/eja+++grt27fH9OnT6zy+d+/e6N69u+0ARJ8+fep8/4EHHkBBQQFGjhyJiIgInDt3DkuXLkWvXr1sPbhevXpBo9HgzTffRHFxMXQ6HUaOHIng4GAsW7YMU6ZMQZ8+fXD33XcjKCgI58+fx08//YRBgwbh3//+dwvfnrZ16623YvXq1dDr9ejatStSUlKwefPmelPvunbtiuHDhyMxMRH+/v7Yt28f1q5dW2d9lffffx+DBw9Gjx49MGvWLMTGxiI7OxspKSm4ePFii+ZCW7399tsYN24cBgwYgJkzZ9qmn+n1+iaffp6YmAgAePzxx5GcnAyNRoO777670efceuutWLduHW6//XbccsstOHv2LJYvX46uXbuitLS0Sdt7/vnncffdd8PV1RXjx49vdCQ2ZMgQvPHGG9Dr9ejRowcAyz+o8fHxOHnyZJPWi2nOdsVgT5233norXnnlFUyfPh0DBw7EkSNH8Nlnn9n+srTXs88+i7Vr12Ly5MmYMWMGEhMTUVBQgB9++AHLly9HQkICHnzwQaxYsQLTpk3D/v370b59e6xduxa7du3CkiVL4O3t3eyf2Z59sDX2faspU6bg66+/xkMPPYStW7di0KBBMJlMSE1Nxddff40NGzbYuictYs/UnfXr1wszZswQOnfuLHh5edmWTnjssceE7OzsBp/z1ltv1Zv2aLV27Vph7NixQnBwsKDVaoWoqChh9uzZwqVLl+o87sMPPxRiY2MFjUZTbxrm1q1bheTkZEGv1wtubm5CXFycMG3aNGHfvn22x0ydOlXw9PSst/1hw4YJ3bp1q3d/dHT0Dac2CkLTT3O/3uOuncpWWFgoTJ8+XQgMDBS8vLyE5ORkITU1td7j/vGPfwj9+/cXfH19BXd3d6Fz587Ca6+9Vm8K7JkzZ4T7779fCA0NFVxdXYV27doJt956q7B27dpW+9k2b94sDBo0SHB3dxd8fHyE8ePHC8ePH6/zGOvUy9zc3HrPr6mpER577DEhKChIUKlUtilw1mlvb7/9dr3nmM1m4fXXXxeio6MFnU4n9O7dW/jxxx+FqVOnCtHR0fV+jmun0b766qtCu3btBLVa3aRpmD/99JMAQBg3blyd+x944AEBgPDRRx/Ve449223q/tGQ6029bGh/t34Oza2zoamXTz/9tBAWFia4u7sLgwYNElJSUupNLbRnuYT8/Hxhzpw5Qrt27QStVitEREQIU6dOFfLy8myPyc7Otv2eaLVaoUePHvVeu7H959qfuTn7oCA0bd9v6nIJVVVVwptvvil069ZN0Ol0gp+fn5CYmCi8/PLLQnFxcZ3am7u0hqr2BRzmX//6F5566ilkZGQ0OOOAiIgcz6FhLwgCEhISEBAQ0OC8dyIiahutMhvnWmVlZfjhhx+wdetWHDlyBN9//70jNkNERE3kkJF9RkYGYmJi4Ovri0ceeQSvvfZaa2+CiIjs4PCePRERiY+XJSQiUgCGPRGRAjjkAC21DrPZjKysLHh7e0vmdHtSJkEQUFJSgvDw8HqLlJFzYNg7saysrHqLIBE5swsXLiAiIkLsMqgBDHsnZj31+8KFC/Dx8RG5GqLrMxgMiIyMbNZyBdQ2GPZOzNq68fHxYdiTJLDd6LzYXCMiUgCGPRGRAjDsiYgUgGFPRKQADHsiIgVg2BMRKQDDnohIARj2REQKwLAnIlIAhj0RkQIw7B1o+/btGD9+PMLDw6FSqfDdd9+JXRIRKRTD3oHKysqQkJCA999/X+xSiEjhuBCaA40bNw7jxo0TuwyiFsstMaKy2oRALx3ctRqxy6Fm4MjeiRiNRhgMhjo3ImcwfdUfGPLWVvyeni92KdRMDHsnsmjRIuj1etuNFy4hZ6GpvfqUySyIXAk1F8PeicyfPx/FxcW224ULF8QuiQgAoKldpr6GYS9Z7Nk7EZ1OB51OJ3YZRPVo1Ja0NwsMe6niyJ6Ibsga9mzjSBdH9g5UWlqKtLQ029dnz57FwYMH4e/vj6ioKBErI7IPw176GPYOtG/fPowYMcL29dy5cwEAU6dOxapVq0Sqish+ahXDXuoY9g40fPhwCOxxkgy4WEf23J8liz17IrohtnGkj2FPRDfENo70MeyJ6IZcNAx7qWPYE9ENcWQvfQx7IrohF55UJXkMeyK6IXVt2HO5BOli2BPRDWnYxpE8hj0R3ZD1AK2ZYS9ZDHsiuiHrAVq2caSLYU9EN8RVL6WPYU9EN8QzaKWPYU9EN8QDtNLHsCeiG+LIXvoY9kR0Qxqueil5DHsiuiGO7KWPYU9EN8S1caSPYU9EN+TCkb3kMeyJ6IbUDHvJ42UJZcRkFpB62YDKahO6hPnAQ8uPl1oHD9BKH9NAJlIvG/D4F3/iVHYpAMBb54KHhsfhoWFxtl9Uouay7kLMeuliG0cGLhVX4L4P9+BUdik8tBoEeetQYqzB2xtOYvbq/aiqMYtdIkmcChwwSB3DXgZe/fE48suq0CXMBzv/PhJ75o/C23f2hM5Fjc0nsvHU1wchcEhGLaCyjey5H0kVw17izuaV4ecjl6FSAf+8KwH+nlqo1SpM7huJj6b2g6tGhZ8OX8JHO8+KXSrJAKNeuhj2EvfV3gsAgBHxwegS5lPne4M7BuLFW7sCAN78JRVpOSVtXh/JCwf20sWwl7gtqdkAgNt7t2vw+1NuisbIzsGoNgn4n2+P8s9wahZVbR+He490MewlLNtQiVPZpVCpgCEdAxt8jEqlwsu3dYO7qwZ/nC3AhmPZbVwlyYH18CwHC9LFsJewgxeKAACdQ33g66G97uMi/T0wc3AMAODdTad4aTmym+0ArbhlUAsw7CUs9ZKlB9/1ml59Qx4YEgNvnQtOZpdg/dHLji6NZIYTL6WPYS9hJ7MNAIDOod43fKyvhxbTB7UHAKzazZk5ZB8Vh/aSx7CXsIuFFQCA6ACPJj3+vpui4aJWYW9GIY5lFTuyNJKZK1nPtJcqhr2EXSquBACE6d2b9PgQHzfc3D0UAPDp7+cdVhfJz5UDtKKWQS3AsJeoapMZeaVGAECo3q3Jz7u3fxQA4Ocjl7iMAtmNYS9dDHuJKiyvgiBY/rwO8Lz+TJxrJcUGINBLh+KKauxKy3NghSQrtnn2THupYthLVGllDQDAS+diW2u8KTRqFW7pYWnl/Hj4kkNqI/nhbBzpY9hLVKnREvbeOvtXqU6u7dtvP53Lk2SoSVRc4ljyGPYO9v7776N9+/Zwc3NDUlIS/vjjj1Z5XdvI3s3+sE+M9oObqxq5JUak5ZS2Sj0kb9Yljpn10sWwd6CvvvoKc+fOxYIFC3DgwAEkJCQgOTkZOTk5LX7timoTAMDdVWP3c3UuGvRr7w8ASEnPb3EtJH8c2Usfw96B/vnPf2LWrFmYPn06unbtiuXLl8PDwwP/93//1+LXtq54YE+//mq9In0BACcucSVMurErexnTXqoY9g5SVVWF/fv3Y/To0bb71Go1Ro8ejZSUlBa/vrl2iKVWNS/srZcqPMM2DjUBR/bSx2vQOkheXh5MJhNCQkLq3B8SEoLU1NQGn2M0GmE0Gm1fGwyG676+YAv75tW3ZPNpAMAfGQXNewFSFPbspY8jeyeyaNEi6PV62y0yMvK6j7W2cVTNHNlb9YzQt+j5RCQNHNk7SGBgIDQaDbKz664fn52djdDQ0AafM3/+fMydO9f2tcFguG7gW0f0zV2uOPXVm7HjdB4GxAU06/mkMLwGreRxZO8gWq0WiYmJ+PXXX233mc1m/PrrrxgwYECDz9HpdPDx8alzu+7ru1g+uipT85Y8cHPVYEzXEHg1Y54+KY9tbRxRq6CW4G+6A82dOxdTp05F37590b9/fyxZsgRlZWWYPn16i19b52KZcmms5vo25Hi2yxIy7SWLYe9Af/3rX5Gbm4uXXnoJly9fRq9evfDLL7/UO2jbHC0d2RPZo6UTAkh8DHsHmzNnDubMmdPqr6urDXtj7clVRI5kHdE3d6oviY89e4mytnEquUwxtQHreR3Meuli2EuUdU0c64JoRI5kbdW3dKoviYdhL1E+tWFfVWNGJVs55GBm9uwlj2EvUZ5aF9svnqGyWtxiSPZsJ/FxZXvJYthLlFqtgrebKwDAUMFWDjmYdWTPxJAsfnQS5uNuaeVwZE+O1lrLc5B4GPYS5mMb2TPsybFss3FEroOaj2EvYb4elrAvLK8SuRKSO86zlz6GvYQFeekAAHklDHtyLM7GkT6GvYQF1oZ9bqnxBo8kahmO7KWPYS9hQd7WkT3DnhzLOrJn0166GPYSxpE9tRXrGbQc2UsXw17CrCP7XI7sycHYs5c+hr2E2Ub2DHtyMIFn0Eoew17CrCP7gvIqVHNde3IggWfQSh4/OgkL8NRCq1FDEIDLxZVil0MyxjNopY9hL2FqtQphvm4AgKyiCpGrITnjGbTSx7CXuHC9OwAgq5hhT47DefbSx7CXuHZ+lrDPLGTYk+PwGrTSx7CXuHa+tWFfxJ49OQ579tLHsJe4K2HPkT05jgBeg1bqGPYSZ23j8AAtOZJ1Zi979tLFsJe4cN8rPXtrX5WotZnMlrR30TDspYphL3Hhvm5QqYCKahPySrnUMTlGTW3T3oVHaCWLYS9xOheNbfrlufwykashuaoxWcJew1NoJYufnAy0D/QAAJzNY9iTY1hH9q4c2UsWw14G2gd4AgAyOLInB6mpPUKrYc9eshj2MhATaA37cpErIbkysWcveQx7GYi2juzZxiEHuXKAlpEhVfzkZCCmtmefkVfG6ZfkEDWceil5DHsZiPT3gFoFlFVx+iU5hnU2Dkf20sVPTgZ0LhrbyVU8SEuOwHn20sewlwnrjJyzuQx7an3WsNcw7CWLYS8THYK9AABpuaUiV0JyxOUSpI9hLxPWsD+dXSJyJSRH1ezZSx4/OZnoFOINADiVzZE9tT7bPHuO7CWLYe8gr732GgYOHAgPDw/4+vo6fHsda0f2mUUVKDPWOHx7pCzWM2h5gFa6GPYOUlVVhcmTJ+Phhx9uk+35eWoR6KUDAKTlcHRPrYsHaKXPRewC5Orll18GAKxatarNttkx2At5pUaczilFQqRvm22X5M/axnHVcHwoVfzknIjRaITBYKhzs0enEB6kJceoti6ExpG9ZDHsnciiRYug1+ttt8jISLue37H2IO1ptnGolXEhNOlj2Nth3rx5UKlUjd5SU1Ob/frz589HcXGx7XbhwgW7nm89SHuKI3tqZbYzaNnGkSz27O3w9NNPY9q0aY0+JjY2ttmvr9PpoNPpmv186/TLi4WWGTmeOn681DqurI3Dkb1UMQ3sEBQUhKCgILHLuC7rjBzrQdpePEhLraSG8+wlj3+TOcj58+dx8OBBnD9/HiaTCQcPHsTBgwdRWurYfnqXMMvo/sQl+w7uEjWm2jbPnpEhVRzZO8hLL72Ejz/+2PZ17969AQBbt27F8OHDHbbdruE+2HE6D8eyih22DVKeqhpL2OtcGPZSxU/OQVatWgVBEOrdHBn0ANA1zAcAcDyLI3tqPcYaEwBAy7CXLH5yMtMt3BL2qZdLbNPliFqixmSGdVfiyF66+MnJTEygF9xc1SivMuEcL2RCraCqtl8PcGQvZfzkZEajViE+tLaVw4O01Aqs/XoA0HKevWTxk5Mh9u2pNVnDXq3iSVVSxk9Ohqx9e47sqTUYa8OeLRxp46cnQ13DObKn1mO0TbvUiFwJtQTDXoY6h3pDpQJySozILTGKXQ5JXBVH9rLAT0+GPLQuiAn0BMAzaanlrLNxeHBW2vjpyVS3cD0A4Egmz6SlljFWW06o4hx7aeOnJ1M921nC/vDFInELIcmzjewZ9pLGT0+mekZYwv7QBY7sqWW4Lo488NOTqe7t9FCrgMuGSuQYKsUuhySMB2jlgZ+eTHnqXNAx2LLc8aGLHN1T87GNIw/89GTsSiunSNxCSNKM1ZyNIwf89GQsofZKVYd4kJZawGjiSVVywLCXsYQIXwDA4YvFEAQud0zNw569PPDTk7H4UG9oNWoUV1TjXH652OWQRDHs5YGfnoxpXdS2dXLYyqHm4lWq5IGfnswlRFhPruKMHGoe28ieB2gljZ+ezPW09e2LRK2DpKuydjaOu5YHaKWMYS9z1hk5RzKLUXPV5eWImqqidm0cd1eGvZQx7GUuNtAT3joXVFabcTK7ROxySIIqGfaywLCXObVahV5RvgCAA+cKxS2GJKm8qgYA2zhSx7BXgMRoPwDAPoY9NUOFtWfPkb2kMewVoG+0PwBgP8OemqGyqraNw5G9pDHsFaBXlC/UKuBiYQWyuQIm2YkHaOWBYa8AXjoXdA61nFzF0T3Ziz17eWDYK4S1b8+wJ3tVsmcvCwx7hejbngdpqXlsbRyO7CWNYa8QfaIsYX8ss9g2b5qoKSqq2LOXA4a9QkT4uSPYW4cas8CLmVCTmc2CbWTvxrCXNIa9QqhUKlsrZ/95tnKoaYw1V5bY8GAbR9IY9gpibeXsz2DYU9NUXNXy48he2hj2CtK3fe3JVecLeeUqahJr2Gtd1NCoVSJXQy3BsFeQrmE+0LmoUVRejfS8MrHLIQmosM6x56he8hj2CqJ1UduuS7svo0DcYkgSKqosPXv266WPYe8AGRkZmDlzJmJiYuDu7o64uDgsWLAAVVVVYpeG/jGWVs6edIY93RiXSpAPF7ELkKPU1FSYzWasWLECHTp0wNGjRzFr1iyUlZXhnXfeEbW2pFh//HsrsOcsw55ujNMu5YNh7wA333wzbr75ZtvXsbGxOHnyJJYtWyZ62CdG+8FFrUJmUQUuFJQj0t9D1HrIuVVwXRzZYBunjRQXF8Pf37/RxxiNRhgMhjq31uahdUHP2ouQ/56e3+qvT/LCNo58MOzbQFpaGpYuXYrZs2c3+rhFixZBr9fbbpGRkQ6pJyk2AABbOXRjpUZL2Hvp2ASQOoa9HebNmweVStXoLTU1tc5zMjMzcfPNN2Py5MmYNWtWo68/f/58FBcX224XLlxwyM+RZD1Ie5Yje2pcaaWljePJsJc8foJ2ePrppzFt2rRGHxMbG2v7/6ysLIwYMQIDBw7EBx98cMPX1+l00Ol0LS3zhvq294dGrcKFggpkFlWgna+7w7dJ0lRmtIS9l45tHKlj2NshKCgIQUFBTXpsZmYmRowYgcTERKxcuRJqtfP8EeWlc0H3dnoculCEPen5mNQnQuySyEmVWsPejVEhdc6TQDKSmZmJ4cOHIyoqCu+88w5yc3Nx+fJlXL58WezSbG7ifHtqAmvYs40jffwEHWDTpk1IS0tDWloaIiLqjpqdZU2apFh/rNiejt/Zt6dGWNs43gx7yePI3gGmTZsGQRAavDmLvu39oVYB5/LLcbmYFyGnhnFkLx8Me4XycXNFt3DLfHvOyqHrYdjLB8NewaxTMHlyFV0P2zjywbBXsJusJ1fxIC1dB+fZywfDXsH6xfhDpQLS88qQY2Dfnurj1Ev5YNgrmN7dFV3DfAAAKWzl0DUEQUBZFZdLkAuGvcIN6hAIANiVlidyJeRsKqvNMJktM8jYxpE+hr3CDYyz9O13peU71dRQEp+1haNSAR5c9VLyGPYK1z/GH64ay/r25/LLxS6HnIht2qXWBWpebFzyGPYK56F1QZ8oPwDATrZy6CpXFkFjC0cOGPbEvj016MoJVWzhyAHDnmxhn5KebzsgR2SdY8+RvTww7AkJEXp46VxQVF6N41mtfylEkqayKs6xlxOGPcFFo8ZNsZalE9i3JysDR/aywrAnAFdaObvPMOzJwlBRDcBy8h1JH8OeAACDa8P+j7MFqKw2iVwNOYNihr2sMOwJANAh2AvB3joYa8w4cL5Q7HLICXBkLy8MewIAqFQqWytn52m2cujKyN6HYS8LDHuysYb9DoY9gW0cuZHtYfbdu3dj8+bNSE9PR3l5OTw8PBAbG4uxY8fipptuErs8pzS0oyXsj2QWI6/UiEAvncgVkZg4spcXSY/sjx07hpqamjr35eXlYfjw4Rg6dCjWrVuHsrIyfP/998jLy8P27dsxatQojB49Gvn5XNL3WsE+buhSu+QxWznEkb28SDrsb7rpJpw/f77OfbNnz4ZWq8WFCxdw8OBBrFmzBlqtFu+99x42b96M7OxsBAQE4KGHHhKpauc2rFMQAGD7qVyRKyGxMezlRdJtnOPHjyM8PLzOfRs2bMD+/fsRFhbW4HO8vLzwj3/8A717926LEiVnaKdALN92BttP58JsFrjaoUKZzAJKak+q8nFj2MuBpEf2kZGR0GjqLtIUEBCA06dPN/q8jIwM+Pv7O7I0yeob7Q8PrQZ5pVU4folLJyiVdV0cgCN7uZB02Ddk3rx5uOeee/D8889j9+7dyM7OhkqlQnFxMdLS0rBy5UpMnToVL7zwgtilOiWti9p2QZNtbOUolrWF4+6qgdZFdjGhSLL7FB9++GF8/PHH2LBhAwYPHozw8HCUlpaib9++iI+Px7Jly/C///u/ePDBB8Uu1WkNZd9e8divlx9J9+yvZ9KkSZg0aRKKiorqTb309fUVuzynZz1Iu/9cIUqNNVwIS4EY9vIj699iX19f9OnTR+wyJCc6wBPRAR44l1+O3Wl5GNstVOySqI1dmWMv64hQFNm1cah12KZgnmYrR4k4spcfhj01aGhHS9hvO5ULQeDVq5TGUMmzZ+WGYU8NGhAXAFeNChcKKpCRXy52OdTGOLKXH4Y9NchT54K+0ZZzEbam5ohcDbU1hr38MOzpukZ2DgYAbD3JsFeawrIqAICfh1bkSqi1MOzpukbUhv2e9AKUGWtu8GiSk/zasPf3ZNjLBcOerisuyBNR/h6oMpl5IXKFKWTYyw7Dnq5LpVLZWjlbTrCVoyQFDHvZYdg7yG233YaoqCi4ubkhLCwMU6ZMQVZWlthl2e3qvj2nYCqD2SygsJxhLzcMewcZMWIEvv76a5w8eRLffPMNzpw5gzvvvFPssuyWFGtZBTOnxIhjWVwFUwmKK6phrv13nQdo5YPnQjvIU089Zfv/6OhozJs3DxMnTkR1dTVcXaUznU3nosGgDoHYdDwbW1Jz0L2dXuySyMEKakf13joXrngpI/wk20BBQQE+++wzDBw4UFJBb2Xr23O+vSLY+vVeHNXLCcPegf7+97/D09MTAQEBOH/+PL7//vtGH280GmEwGOrcnMGIeEvYH7pYhLxSo8jVkKMVcI69LDHs7TBv3jyoVKpGb6mpqbbHP/vss/jzzz+xceNGaDQa3H///Y0e5Fy0aBH0er3tFhkZ2RY/1g2F6t3QLdwHggD8dpILo8mdNewDeHBWVlQCp1g0WW5uLvLz8xt9TGxsLLTa+r8kFy9eRGRkJHbv3o0BAwY0+Fyj0Qij8crI2WAwIDIyEsXFxfDx8WlZ8S20eONJLN2Shlt6hOH9+7hstJy9vzUNb284iTsTI/DO5IQmPcdgMECv1zvFvkoN4wFaOwQFBSEoKKhZzzWbzQBQJ8yvpdPpoNPpmvX6jjaiczCWbknD9lO5qDaZ4arhH4VyxZG9PDHsHWDPnj3Yu3cvBg8eDD8/P5w5cwYvvvgi4uLirjuqd3YJEb4I8NQiv6wKezMKMDAuUOySyEFsPXuGvaxweOYAHh4eWLduHUaNGoX4+HjMnDkTPXv2xLZt25x25H4jGrUKw2sP1P7Ks2lljWfPyhNH9g7Qo0cPbNmyRewyWt2YrsH45sBFbDqejRdu6QKVSiV2SeQAtrDnbBxZ4ciemmxIxyBoXdQ4X1COk9klYpdDDsJ59vLEsKcm89S5YEgHS69+07FskashR+HIXp4Y9mSXMV1DAACbTjDs5ai8qgYV1SYAQABH9rLCsCe7jOoSApUKOHyxGJeKK8Quh1pZjsEyNdjdVQMvHQ/pyQnDnuwS5K1Dnyg/AMDm4xzdy01u7XIYQd46HoCXGYY92c3aytnIsJed3BJL2Ad7S3OKMF0fw57sNrY27H9Pz4ehslrkaqg15RgqAVhG9iQvDHuyW2yQF+KCPFFtErgwmsxY2zgc2csPw56aZUzXUADAJrZyZMV6gJYje/lh2FOzjO1maeX8lpqDqhqzyNVQa7kysncTuRJqbQx7apZeEb4I8tahxFiD39MbX/aZpIMje/li2FOzqNUqjO5iWRiNrRz5uHrqJckLw56abexVfXuzmdfAkTqTWUC+tY3jw7CXG4Y9NduAuAB4ajW4bKjEoYtFYpdDLZRfZoRZANQqIMCTYS83DHtqNjdXDUZ1sRyoXX/0ssjVUEtZ+/UBXjpo1Dx7Vm4Y9tQif+lhaeX8fORSoxdTJ+dn69d7cVQvRwx7apFhnYLh7qrBxcIKHM00iF0OtUCugf16OWPYU4u4azUY0dlyEfafj14SuRpqCY7s5Y1hTy02rnsYAGA9WzmSll27Lg5H9vLEsKcWG9E5GDoXNTLyy5F6mZcrlKpLxZawD9W7i1wJOQLDnlrMS+eCYZ0srZz1R9jKkSrrxWjC9VwqQY4Y9tQq/tLD0sr5mVMwJetSkWVkH8aRvSwx7KlVjOwSDFeNCmk5pTidzVaO1FRWm5Bfe6HxcF+O7OWIYU+twsfNFUM61rZyOLqXnMu1/Xp3Vw307q4iV0OOwLCnVjOu+5UTrEhasmr79WG+brz2rEwx7KnVjOkaAhe1CqmXS5CeWyp2OWQHa78+nP162WLYU6vx9dBiYIdAAGzlSI11Jk4YZ+LIFsOeWpW1lbOeZ9NKSlZtzz7MlyN7uWLYU6sa2zUEGrUKRzMNyMgrE7scaqJLRRzZyx3DnlpVgJcOA+MCAAD/PZQlcjXUVNazZxn28sWwp1Z3W0I4AOCHQ1lcK0cismpH9uFs48gWw55aXXL3UGhd1DidU8q1ciSgzFgDQ2UNAI7s5YxhT63Ox80VI+ItJ1j9wFaO07POxPHWucDbjSdUyRXDnhzitoR2ACx9e7ZynFuWdU0cLpMgawx7coiRnYPhqbVcwerA+SKxy6FGXCgsBwBE+HmIXAk5EsOeHMJdq8GYrpaLkXNWjnM7X2AJ+yh/hr2cMewdzGg0olevXlCpVDh48KDY5bSp23pZZuX8ePgSakxmkauh67lQG/aRDHtZY9g72HPPPYfw8HCxyxDF4A5B8PVwRV6pEb+nF4hdDl3HhQLLAVqO7OWNYe9A69evx8aNG/HOO++IXYootC5q2/VpfziUKXI1dD1s4ygDw95BsrOzMWvWLKxevRoeHk37JTIajTAYDHVuUmc9wWr90csw1phEroauVVxejeKKagBAhB9PqJIzhr0DCIKAadOm4aGHHkLfvn2b/LxFixZBr9fbbpGRkQ6ssm30j/FHiI8OJZU12HYyV+xy6BrWmTiBXlp46lxEroYciWFvh3nz5kGlUjV6S01NxdKlS1FSUoL58+fb9frz589HcXGx7XbhwgUH/SRtR6NW4daeV5ZPIOfCg7PKwX/K7fD0009j2rRpjT4mNjYWW7ZsQUpKCnQ6XZ3v9e3bF/fddx8+/vjjBp+r0+nqPUcObksIx0c7z2LziWyUGWs4gnQi7NcrB3/r7BAUFISgoKAbPu69997DP/7xD9vXWVlZSE5OxldffYWkpCRHluiUekboER3ggXP55dh8IhsTerUTuySqxbBXDoa9A0RFRdX52svLCwAQFxeHiIgIMUoSlUqlwoSEcLy3JQ3f/pnJsHci1rCP5NmzsseePbWJib0tAb/9VC5ySipFroasLhZa5tizZy9/DPs20L59ewiCgF69eoldimhig7zQO8oXZgH44SAP1DoDk1nAxdrZOFEBDHu5Y9hTm5nUx9LC+uYAT7ByBpcNlag2CXDVqBDqwxUv5Y5hT21mfM8wuGpUOHHJgONZ0j9hTOrO5lquERzp5wGNWiVyNeRoDHtqM74eWozqbFkJ89s/L4pcDaXnlQKwtNhI/hj21KYm9bEcqP3uYBZXwhTZmRxL2McFeYpcCbUFhj21qeHxwfDzcEVuiRG7zuSLXY6ipedZ2jixDHtFYNhTm9K6qDG+dnG0dQfYyhFTem3PPo5tHEVg2FObs87K2XDsMkoqq0WuRpnKq2qQWWSZY8+evTIw7KnNJUToERvkicpqM34+cknschTpbG0Lx9fDFf6eWpGrobbAsKc2p1KpcEft6H7NPrZyxGBt4cQGsl+vFAx7EsWdiRFQq4B95wqRVjsrhNoO+/XKw7AnUYT4uGFEfDAAYM1+6a/bLzVncjnHXmkY9iSayX0tV+L6Zn8mqjnnvk1dOaGKbRylYNiTaEZ1CUaglxZ5pUb8xksWthlBEK5q4zDslYJhT6Jx1ahxe+3Sx1/tZSunrVw2VKK8ygSNWoUof4a9UjDsSVR31bZytp7M4Tr3beRUtqWFEx3gAa0LI0Ap+EmTqDqGeKN3lC9MZgHruPRxmzh52bLiaOdQb5ErobbEsCfR/bV2dP/1vgsQBEHkauQv9VIJAKBzqI/IlVBbYtiT6G7pGQZ3Vw3Sc8uw/1yh2OXIXuplS9jHc2SvKAx7Ep23mytu6RkGAPjiDx6odaQak9l2ElsXjuwVhWFPTuGe/lEAgB8PZ6G4nIujOcrZvDJUmczw0GoQ4ecudjnUhhj25BT6RPmic6g3jDVmrOXSxw5zdQtHzUsRKgrDnpyCSqXCfTdFAwA+23OOB2odJJUzcRSLYU9O4/be7eCptRyo/T29QOxyZOmkdWQfwrBXGoY9OQ0vnQsm1J5R+9mecyJXI0/WNk7nMB6cVRqGPTmVe2sP1G44dhm5JUaRq5GXkspqXCy0XJ2KbRzlYdiTU+neTo9ekb6oNgn4eh+nYbamU9mWUX2ojxt8PXh1KqVh2JPTuS/JMrr/4o/zMJl5oLa1HM20HJztEsZRvRIx7MnpjE8Ih4+bCy4WVmD7aS593FoOXywGAPSI8BW3EBIFw56cjpurBncmWtbL+ex3HqhtLYcvFgGwXPCdlIdhT07p3tpWzq+pObhQUC5yNdJXZqxBWu2lCHsw7BWJYU9OqUOwF4Z0DIQgAB/vzhC7HMk7mlkMQQDC9G4I9nYTuxwSAcOenNaMQTEAgK/2XUCZsUbkaqTtSGZtv74dR/VKxbAnpzWsUxBiAj1RUlmDb7heTotYD872ZAtHsRj25LTUahWmDrCsl7NqdwbMnIbZbNaDsz05E0exGPbk1O7sGwlvnQvSc8s4DbOZisurkZFvOcjNNo5yMezJqXnpXDC59rKFK3dliFuMRFn79VH+HvDz5JmzSsWwd5D27dtDpVLVub3xxhtilyVJ0wa2h0oFbDuVa7vKEjXd4cwiAJxyqXQMewd65ZVXcOnSJdvtscceE7skSYoK8MCoziEAgE9SMsQtRoIO1F7Xtxf79YrGsHcgb29vhIaG2m6enp5ilyRZMwa1BwCs3X8RxRW8bGFTmc0C9mZYwr5fjL/I1ZCYGPYO9MYbbyAgIAC9e/fG22+/jZqaxueKG41GGAyGOjeyGBAXgPgQb5RXmfDlH+fFLkcyTueUoriiGu6uGnQL5xr2Ssawd5DHH38cX375JbZu3YrZs2fj9ddfx3PPPdfocxYtWgS9Xm+7RUZGtlG1zk+lUmHmYMtJVv+36yyMNSaRK5KGvRmWK371jvKFq4a/7krGT98O8+bNq3fQ9dpbamoqAGDu3LkYPnw4evbsiYceegiLFy/G0qVLYTRe/4Ic8+fPR3Fxse124QLXc7/ahN7hCPVxQ7bBiO/+zBS7HEmwhn2/9mzhKJ2L2AVIydNPP41p06Y1+pjY2NgG709KSkJNTQ0yMjIQHx/f4GN0Oh10Ol1Ly5QtnYsGMwfH4LWfT2DF9nRMToyEWq0SuyyntvesJez7s1+veAx7OwQFBSEoKKhZzz148CDUajWCg4NbuSpluScpCku3nEZ6bhk2Hs/Gzd1DxS7JaWUWVSCruBIatQq9In3FLodExjaOA6SkpGDJkiU4dOgQ0tPT8dlnn+Gpp57C3/72N/j5+YldnqR56VwwpXYJhWXbzkAQuITC9VhH9d3DfeCp47hO6Rj2DqDT6fDll19i2LBh6NatG1577TU89dRT+OCDD8QuTRamDYyBzkWNQxeK8Ht6gdjlOK0/2K+nq/Cfewfo06cPfv/9d7HLkK0gbx0m943Ap7+fx/JtZzAgLkDskpySdWTfl2FP4MieJOrBIXFQ1y6hcDyL5yNcK8dQidM5pVCpeHCWLBj2JElRAR64pWc4AGD5tjMiV+N8dpzOA2BZ5dKfi58RGPYkYbOHWqa5/ng4C+fyy0SuxrlYl4Me0jFQ5ErIWTDsSbK6t9NjWKcgmAVg6ZY0sctxGmazgJ21I/uhHZs3VZjkh2FPkvbUmE4AgHUHLuJsHkf3AHD8kgH5ZVXw1GrQO4pTfcmCYU+S1ivSF6M6B8MsAP/afErscpyCtV8/IC4AWhf+ipMF9wSSPOvo/vtDWUjLKRG5GvHtsPXr2cKhKxj2JHnd2+kxtmsIBAFYsvm02OWIqryqBvtq168f2olhT1cw7EkWrKP7n45cQupl5c67/z09H1UmMyL83NE+wEPscsiJMOxJFrqE+eCWHmEQBOBfCh7dbzqeDQAYHh8ElYorgtIVDHuSjSdGd4RKBaw/ehnHsorFLqfNmcwCNh6zhP3N3cJEroacDcOeZKNTiDfG155V++4m5Y3u92UUIL+sCnp3VyTFcokEqothT7Ly+KiOUKuAzSey8ef5QrHLaVMbakf1o7uE8BKEVA/3CJKVDsFeuKNPBADg9Z9PKGa9e0EQsOHYZQBAcrcQkashZ8SwJ9mZO7YT3FzV2JtRaDtgKXdHMw3ILKqAu6uGUy6pQQx7kp0wvTtmDo4BALzxSyqqTWaRK3K8X45dAgCM6BwEN1eNyNWQM2LYkyzNHhYHf08t0nPL8OXeC2KX41CCIGD9UWsLh9fkpYYx7EmWfNxc8cSojgCAdzedQnF5tcgVOc6hi8VIzy2Dm6saIzvzgvbUMIY9yda9SVHoGOyFgrIqLPlVvoukrd1v+cvl5m6h8HZzFbkaclYMe5ItV40aL43vCgD4JOUcTmXLb5G0ymoT/nvI0q+/MzFS5GrImTHsSdaGdAzC2K4hMJkFvPLf47KbivnriRwUV1QjTO/GC69Toxj2JHsv3NIVWhc1dqbl4ZfaA5lyYW3hTOrTDho118Kh62PYk+xFBXjYrle78L/HUFIpj4O1OYZKbK+9UIn1RDKi62HYkyI8OqIDogM8kG0wYvFGeRys/eZAJkxmAX2ifBEb5CV2OeTkGPakCG6uGvxjYncAwMcpGTh0oUjcglqo2mTGJykZAIC7+0eJWwxJAsOeFGNIxyBM6BUOQQDmrzsi6TNrfz5yCZeKKxHopcOEXuFil0MSwLAnRXnhlq7Qu7vi+CUDlv12RuxymkUQBPzfzrMAgCk3RUPnwuUR6MYY9qQoQd46vDKhGwDgvV9PS/IiJ/vPFeLQxWJoXdT4201s4VDTMOxJcW5LCEdytxDUmAU8s+Ywqmqk1c75zw7LqH5S73YI8NKJXA1JBcOeFEelUuEfE3vAz8MVJy4Z8N6v0rmq1dm8Mmw8bjlXYEbtyp5ETcGwJ0UK8tbhtdt7AADe/y0Nu8/kiVxR0yzeeBJmARjVORidQrzFLockhGFPivWXHmH4a99ICALw5JcHkV9qFLukRh3NLMaPhy9BpQKeSY4XuxySGIY9KdqC27qiQ7AXckqMeHrNIZjNzrt2zlsbTgIAJiSEo0uYj8jVkNQw7EnRPLQu+Pe9vaF1UeO3k7n439/SxC6pQbvP5GH7qVy4qFWYO4ajerIfw54Ur3OoD165zTIdc/GmU/j1hHNdt9ZkFvDm+lQAljX6owI8RK6IpIhh70A//fQTkpKS4O7uDj8/P0ycOFHskug67u4fhSk3RUMQgCe+PIi0HOdZ+37V7gwculgML50L5ozsIHY5JFEMewf55ptvMGXKFEyfPh2HDh3Crl27cO+994pdFjXipfFd0T/GH6XGGsz6ZD8KyqrELgnn88vxTm2vfv5fOiPY203kikiqVILcrubgBGpqatC+fXu8/PLLmDlzZrNfx2AwQK/Xo7i4GD4+PCDXFvJLjbjt37uQWVSBhAg9Ppt1E7x0LqLUIggC/vbRHuxKy8dNsf74/IGboHbSNeu5rzo/juwd4MCBA8jMzIRarUbv3r0RFhaGcePG4ejRo2KXRjcQ4KXDxzP6w8/DFYcuFuOh1fthrDGJUsuXey9gV1o+3FzVeGNST6cNepIGhr0DpKenAwAWLlyIF154AT/++CP8/PwwfPhwFBQUXPd5RqMRBoOhzo3aXodgL6ya3h8eWg12puXhiS8OtvmSCn+eL8SCH44BAJ4eE4/2gZ5tun2SH4a9HebNmweVStXoLTU1FWazJRief/553HHHHUhMTMTKlSuhUqmwZs2a677+okWLoNfrbbfISF5AWiwJkb74YEpfaDVq/HLsMmav3ofK6rYZ4WcbKjF79X5U1ZgxuksIZnJZBGoF7NnbITc3F/n5+Y0+JjY2Frt27cLIkSOxY8cODB482Pa9pKQkjB49Gq+99lqDzzUajTAar5zFaTAYEBkZyT6oiLadyq0NejOSYvzxn6l94e3m6rDtVVab8NcVKTh0sRidQrzwzcMDHbq91sKevfMT58iTRAUFBSEoKOiGj0tMTIROp8PJkydtYV9dXY2MjAxER0df93k6nQ46HVcxdCbDOgXhkxlJmLlqL/acLcCdy1KwfEoiYhzQVikz1uChT/fj0MVi+Hq44sP7HfsPCykL2zgO4OPjg4ceeggLFizAxo0bcfLkSTz88MMAgMmTJ4tcHdmrf4w/Pp91EwK9dDiZXYLb/r0Tm4+37olXReVV+NtHe7DjdB48tBos/1siogPYp6fWw7B3kLfffht33303pkyZgn79+uHcuXPYsmUL/Pz8xC6NmqFHhB4/PT4YidF+KKmswQOf7MOC74/CUFnd4tdOzy3F3R/8jj/PF0Hv7orPHkjCTbEBrVA10RXs2Tsx9kGdT1WNGa/9dBwfp5wDAAR76/DCrV0xvmcYVCr7pkaazAI+2pmOxRtPwVhjRrC3DqtnJiE+VHpLF3NfdX4MeyfGXyDntfN0Hl78/ijO5pUBADoGe2HG4Bjc3rsd3FwbvyasscaEX45exn92nMWRTMtlEYd0DMQbd/REO193h9fuCNxXnR/D3onxF8i5VVabsGJbOlZsP4PyKsu0TG83FyTFBOCmWH90b6eHl84FHloNiiuqcfJyCY5fMuDnI5eQV2pZisFb54IXbu2Cu/pG2v2XgTPhvur8GPZOjL9A0lBcUY01+y5g5a4MZBZVNOk5IT463N0vCvfdFCWL9W64rzo/hr0T4y+QtJjMAg5fLMKeswXYk56Ps3llKK8yoaLKBDetBvEh3ogP9Ub/GH+M7BwMV4185kdwX3V+DHsnxl8gkgruq85PPkMLIiK6LoY9EZECMOyJiBSAYU9EpAAMeyIiBWDYExEpAMOeiEgBGPZERArAsCciUgCGPRGRAjDsiYgUgGFPRKQADHsiIgVg2BMRKYCL2AXQ9VlXnzYYDCJXQtQ46z7KFdOdF8PeiZWUlAAAIiMjRa6EqGlKSkqg1+vFLoMawIuXODGz2YysrCx4e3u3+vVJDQYDIiMjceHCBcldbIK1t70b1S0IAkpKShAeHg61mt1hZ8SRvRNTq9WIiIhw6DZ8fHwkFTpXY+1tr7G6OaJ3bvwnmIhIARj2REQKwLBXKJ1OhwULFkCn04ldit1Ye9uTat10BQ/QEhEpAEf2REQKwLAnIlIAhj0RkQIw7ImIFIBhr1Dvv/8+2rdvDzc3NyQlJeGPP/4Qu6Qb2r59O8aPH4/w8HCoVCp89913YpfUJIsWLUK/fv3g7e2N4OBgTJw4ESdPnhS7rCZZtmwZevbsaTuZasCAAVi/fr3YZVEzMOwV6KuvvsLcuXOxYMECHDhwAAkJCUhOTkZOTo7YpTWqrKwMCQkJeP/998UuxS7btm3Do48+it9//x2bNm1CdXU1xo4di7KyMrFLu6GIiAi88cYb2L9/P/bt24eRI0diwoQJOHbsmNilkZ049VKBkpKS0K9fP/z73/8GYFmDJzIyEo899hjmzZsncnVNo1Kp8O2332LixIlil2K33NxcBAcHY9u2bRg6dKjY5djN398fb7/9NmbOnCl2KWQHjuwVpqqqCvv378fo0aNt96nVaowePRopKSkiVqYcxcXFACyhKSUmkwlffvklysrKMGDAALHLITtxITSFycvLg8lkQkhISJ37Q0JCkJqaKlJVymE2m/Hkk09i0KBB6N69u9jlNMmRI0cwYMAAVFZWwsvLC99++y26du0qdllkJ4Y9URt69NFHcfToUezcuVPsUposPj4eBw8eRHFxMdauXYupU6di27ZtDHyJYdgrTGBgIDQaDbKzs+vcn52djdDQUJGqUoY5c+bgxx9/xPbt2x2+dHVr0mq16NChAwAgMTERe/fuxb/+9S+sWLFC5MrIHuzZK4xWq0ViYiJ+/fVX231msxm//vor+7AOIggC5syZg2+//RZbtmxBTEyM2CW1iNlshtFoFLsMshNH9go0d+5cTJ06FX379kX//v2xZMkSlJWVYfr06WKX1qjS0lKkpaXZvj579iwOHjwIf39/REVFiVhZ4x599FF8/vnn+P777+Ht7Y3Lly8DsFzsw93dXeTqGjd//nyMGzcOUVFRKCkpweeff47ffvsNGzZsELs0spdAirR06VIhKipK0Gq1Qv/+/YXff/9d7JJuaOvWrQKAerepU6eKXVqjGqoZgLBy5UqxS7uhGTNmCNHR0YJWqxWCgoKEUaNGCRs3bhS7LGoGzrMnIlIA9uyJiBSAYU9EpAAMeyIiBWDYExEpAMOeiEgBGPZERArAsCciUgCGPRGRAjDsSVGWLVuGqKgoeHp6YtKkScjNzRW7JKI2wbAnxVi3bh2effZZLF26FPv27UNJSQnuvPNOscsiahNcLoEUIzExEWPGjMEbb7wBAMjKykJkZCS2bduGwYMHi1wdkWNxZE+KUFhYiAMHDuCWW26x3RceHo7u3btj8+bNIlZG1DYY9qQI6enpAGC7CIdVx44dbd8jkjOuZ0+KUF5eDsAS7lczGo2YMGGCGCURtSmGPSmCh4cHAOC3336Dr6+v7f4nnnjC9j0iOWPYkyLExsYCAHx8fOq0ciorK23fI5Iz9uxJEfz8/JCYmIgdO3bY7istLUVKSgrGjBkjYmVEbYNTL0kxvv32Wzz88MP45JNPEBMTgxdeeAE5OTnYunWr2KURORzbOKQYt99+O7KzszFz5kzk5+fj5ptvxtdffy12WURtgiN7IiIFYM+eiEgBGPZERArAsCciUgCGPRGRAjDsiYgUgGFPRKQADHsiIgVg2BMRKQDDnohIARj2REQKwLAnIlIAhj0RkQL8P1WJGXQ7jkhNAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 初始化动力学模型，准备收集数据\n",
    "params={'m': 2,'L': 1, 'b': 0.1}\n",
    "controller_params = {'K':np.array([[35, 8]])} # 比较差的稳定控制器\n",
    "\n",
    "p1 = InvertedPendulum(system_params = params,\n",
    "                      controller_params = controller_params,\n",
    "                      dt = 0.01, \n",
    "                      controller_period = 0.01)\n",
    "\n",
    "\n",
    "x_initial = torch.tensor([[3],[1]]).to(device)\n",
    "step_num = 4000\n",
    "sim_data_ = p1.simulate_rk4(x_initial, step_num, 1,)\n",
    "p1.convergence_judgment(sim_data_)\n",
    "p1.plot_phase_portrait(data_sim = sim_data_,\n",
    "                       arrow_on = False,\n",
    "                       title = 'System Phase Portrait with initial controller')\n",
    "\n",
    "d1 = DlearningProcess(system=p1,\n",
    "                      actor_bound=5.0,\n",
    "                      n_hiddens_policy=16,\n",
    "                      n_hiddens_lyapunov=16,\n",
    "                      n_hiddens_dfunction=32,\n",
    "                      actor_lr=0.001,\n",
    "                      lyapunov_lr=0.004,\n",
    "                      dfunction_lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = d1.sample_training_data(sample_trajectory_number = 4,\n",
    "                            sample_number_per_trajectory = 300,\n",
    "                            sample_radius = 10,\n",
    "                            sample_number_in_radius = 0,\n",
    "                            invariant_sample = 1,\n",
    "                            sample_plot = 1,\n",
    "                            the_controller = d1.actor.Controller,\n",
    "                            title='Before policy initialization')\n",
    "\n",
    "\n",
    "d1.initialize_policy_net(x_train_lim = 10,\n",
    "                         x_test_lim = 13,\n",
    "                         sample_num = 1000,\n",
    "                         iteration = 10**4,\n",
    "                         lr = 1e-3)\n",
    "\n",
    "\n",
    "x_initial = torch.tensor([[3],[1]]).to(device)\n",
    "step_num = 4000\n",
    "sim_data_ = p1.simulate_rk4(x_initial = x_initial, \n",
    "                           step_number = step_num,\n",
    "                           use_controller = 1,\n",
    "                           the_controller = d1.actor.Controller)\n",
    "stepconverge_initial = p1.convergence_judgment(sim_data_)\n",
    "p1.plot_phase_portrait(data_sim = sim_data_,\n",
    "                       arrow_on = False,\n",
    "                       title = 'System Phase Portrait with initialized NN controller')\n",
    "\n",
    "sim_data = d1.sample_training_data(sample_trajectory_number = 0,\n",
    "                                   sample_number_per_trajectory = 0,\n",
    "                                   sample_radius = 10,\n",
    "                                   sample_number_in_radius = 3000,\n",
    "                                   invariant_sample = 1,\n",
    "                                   sample_plot = 1,\n",
    "                                   the_controller = d1.actor.Controller,\n",
    "                                   title = 'After policy initialization but before LQR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个空列表来存储每次迭代的 stepconverge 值\n",
    "stepconverge_list = []\n",
    "stepconverge_list.append(stepconverge_initial)\n",
    "\n",
    "for i in range(30):\n",
    "    print(f\"第{i+1}次迭代\")\n",
    "    d1.learn_V_LNN(sample_data = sim_data,\n",
    "                iteration = 1*10**4,\n",
    "                plot_loss = True,\n",
    "                plot_lyapuonv = True,\n",
    "                lr = 1e-3)\n",
    "\n",
    "    d1.learn_D_DNN(sample_data = sim_data,\n",
    "                iteration = 10**4,\n",
    "                plot_loss = True,\n",
    "                plot_dfunction = True,\n",
    "                lr = 1e-4)\n",
    "\n",
    "    d1.policy_improvement_PPO(sample_data = sim_data,\n",
    "                        iteration = 10**2,\n",
    "                        plot_loss = True)\n",
    "\n",
    "\n",
    "    sim_data = d1.sample_training_data(sample_trajectory_number = 16,\n",
    "                                    sample_number_per_trajectory = 300,\n",
    "                                    sample_radius = 10,\n",
    "                                    sample_number_in_radius = 0,\n",
    "                                    invariant_sample = 1,\n",
    "                                    sample_plot = 1,\n",
    "                                    the_controller = d1.actor.Controller,\n",
    "                                    title = 'After policy initialization and after policy improvement')\n",
    "\n",
    "    x_initial = torch.tensor([[3],[1]]).to(device)\n",
    "    step_num = 4000\n",
    "    sim_data_ = p1.simulate_rk4(x_initial = x_initial, \n",
    "                            step_number = step_num,\n",
    "                            use_controller = 1,\n",
    "                            the_controller = d1.actor.Controller)\n",
    "    \n",
    "    stepconverge = p1.convergence_judgment(sim_data_)\n",
    "    stepconverge_list.append(stepconverge)  # 将每次迭代的 stepconverge 值添加到列表中\n",
    "\n",
    "    p1.plot_phase_portrait(data_sim = sim_data_,\n",
    "                        arrow_on = False,\n",
    "                        title = 'System Phase Portrait with improved NN controller')\n",
    "  \n",
    "        \n",
    "# 绘制 stepconverge 值随迭代次数变化的折线图\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(stepconverge_list) + 1), stepconverge_list, marker='o')\n",
    "plt.axhline(y=stepconverge_initial, color='r', linestyle='--', label=f'stepconverge_initial = {stepconverge_initial}')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Step Convergence')\n",
    "plt.title('Step Convergence over Iterations')\n",
    "plt.grid(True)\n",
    "plt.legend()  # Show legend with added line\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 保存控制器\n",
    "torch.save(d1.actor.Controller, 'result/controller_DL-Clip.pth')\n",
    "torch.save(d1.lyapunov,'result/lyapunov_DL-Clip.pth')\n",
    "torch.save(d1.dfunction,'result/dfunction_DL-Clip.pth')\n",
    "\n",
    "# 保存 stepconverge_list 到文件\n",
    "with open('result/stepconverge_list_controller_DL-Clip.pk', 'wb') as f:\n",
    "    pickle.dump(stepconverge_list, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
